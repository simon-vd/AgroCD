---
sidebarDepth: 1
---

# üìñ Kubernetes clusters

Before we can properly work with Kubernetes, we need a cluster to use! Several cloud providers offer Kubernetes as a service, but we're going to use [kind](https://kind.sigs.k8s.io/) to create a local cluster to play with first. If you're interested in deploying a multi server solution we'll look into the official Kubernetes distribution [kubeadm](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/) later.

## What is a k8s cluster?

A k8s cluster is a group of machines. These machines are distinct servers, VM's or containers. Each machine is called a node. Nodes can have different roles in k8s:

- "Controller" or "master" nodes that run the Kubernetes API and the control plane
  - You can run multiple controllers (in uneven numbers) to have a highly available cluster
- One or more "worker" node that run the actual workloads (= pods with containers).

![k8s cluster](https://opensource.com/sites/default/files/uploads/kubernetes-architecture-diagram.png)

_(Nived Velayudhan, CC BY-SA 4.0)_

## kind - the developer's friend

In the [Kubernetes quickstart guide](./02.k8s-install.md), you saw how to install kind and create a simple cluster. If you need a cluster it is as easy as running `kind create cluster` and you will have a cluster (containing only one node) running in a few seconds. You can then interact with it with `kubectl` as you would with any other cluster, the credentials are automatically configured.

Check it out with `kubectl get nodes`:

```bash
$ kubectl get nodes
NAME                 STATUS   ROLES                  AGE   VERSION
kind-control-plane   Ready    control-plane,master   10s   v1.22.2
```
::: warning
When you try to use kubectl and get an error message saying the connection got refused, something went wrong when creating the cluster.
:::

When you are done you can delete the cluster with `kind delete cluster`.

:::tip
Having trouble creating the cluster? Check out the following link for help: https://kind.sigs.k8s.io/docs/user/known-issues/#failing-to-properly-start-cluster
:::

### Creating our own cluster

While we can use `kind create cluster` to create a basic cluster with one node, usually we want to create more nodes to balance the workload. We also want to provide our own configuration. While using kind, we can do this using a kindconfig file.

```bash
$ cat kindconfig
kind: Cluster # Specify we are creating a cluster
apiVersion: kind.x-k8s.io/v1alpha4 # Version of the kind config
name: linuxweb-cluster # Set a name for the cluster
nodes:
- role: control-plane # Create a controller node
  kubeadmConfigPatches: # Configure the controller node
  - |
    kind: InitConfiguration
    nodeRegistration:
      kubeletExtraArgs:
        # Enable ingress support
        node-labels: "ingress-ready=true" 
  extraPortMappings: # Create port mappings to communicate
  - containerPort: 80
    hostPort: 80
    protocol: TCP
  - containerPort: 443
    hostPort: 443
    protocol: TCP

$ kind create cluster --config=kindconfig
Creating cluster "linuxweb-cluster" ...
 ‚úì Ensuring node image (kindest/node:v1.25.2) üñº
 ‚úì Preparing nodes üì¶
 ‚úì Writing configuration üìú
 ‚úì Starting control-plane üïπÔ∏è
 ‚úì Installing CNI üîå
 ‚úì Installing StorageClass üíæ
 ‚úì Joining worker nodes üöú
Set kubectl context to "kind-linuxweb-cluster"
You can now use your cluster with:

kubectl cluster-info --context kind-linuxweb-cluster

Thanks for using kind! üòä
```

When we use kind in class it is a good idea to enable ingress support so that we can use HTTP/HTTPS routing. See [Kubernetes resources](./04.k8s-resources.md) for more info.

Want to create a multi-node cluster? Then you can use the following kind configuration file, which sets up 1 controller node and 1 worker node:

```yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
  - role: control-plane
    kubeadmConfigPatches:
      - |
        kind: InitConfiguration
        nodeRegistration:
          kubeletExtraArgs:
            node-labels: "ingress-ready=true"
    extraPortMappings:
      - containerPort: 80
        hostPort: 80
        protocol: TCP
      - containerPort: 443
        hostPort: 443
        protocol: TCP
  - role: worker
```

:::danger EXERCISE
Create a `kind` cluster with 3 worker nodes using the configuration file above. Use the `kind create cluster` to create the cluster. Check the status of the cluster with `kubectl get nodes`.
:::

### What's in a YAML?

Everything in Kubernetes is resource. A resource is like an object in a programming language. It has a type, a name and some properties.
We write these down in YAML files.

Let's take a look at the typical structure Kubernetes uses:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-application
  labels:
    app: company-website-app
spec:
  containers:
    - name: webserver
      image: mycompany/webserver:v1.0.0
```

- `apiVersion` - The version of the Kubernetes API you are using to create this object.
  - This is a required field
  - often you will see `v1` which means it is the CORE API
    - `apps/v1`, `extensions/v1beta1`, `networking.k8s.io/v1` are all other APIs depending on their work domain
    - some resources are not yet stable so you might see `v1alpha1` or `v1beta1` or alike.
    - Kubernetes will store one version of the resource in etcd and convert it to the version you are using when you request it. This allows for upgradign versions without you needing to do much!
- `kind` - is the type of resource it is within the API version
- `metadata` - is data that helps uniquely identify the resource, including a name, multiple labels, and optional namespace.
- `spec` - is the specification of the desired behavior of the resource, this contains all configuration for the resource.

This format is used for all resources in Kubernetes. So we can use the same format for a Pod, Deployment, Service, Ingress, ... and so on, even for third-party resources (called Custom Resources) that any add-on may bring.

An exception is `status`, you will find it when you ask back a resource from Kubernetes. It is technically a "sub-resource" of the resource and contains information about the current state of the resource.

```yaml
apiVersion: v1
kind: Pod
[...]
status:
  conditions:
    lastTransitionTime: "2022-10-03T08:43:55Z"
    status: "True"
    type: Ready
  containerStatuses:
  - containerID: containerd://d8378faef614be496ba8e2ab92f2f1836795f7eebd7b7dd5c5099f7ed83364b5
    image: docker.io/kindest/kindnetd:v20220726-ed811e41
    imageID: sha256:d921cee8494827575ce8b9cc6cf7dae988b6378ce3f62217bf430467916529b9
    lastState: {}
    name: kindnet-cni
    ready: true
    restartCount: 0
    started: true
    state:
      running:
        startedAt: "2022-10-03T08:43:55Z"
  hostIP: 172.20.0.2
  phase: Running
  podIP: 172.20.0.2
  podIPs:
  - ip: 172.20.0.2
  qosClass: Guaranteed
  startTime: "2022-10-03T08:43:53Z"
```

You'll never set this by hand but the system itself will update it. It is a way for Kubernetes to keep track of the current state of the resource.

:::tip
You can always get the YAML spec of a resource by using the `kubectl get <resource type> <name> -o yaml` command.
:::

#### Intermezzo: labels

![labels](labels.png)

_Labels, or name tags is how they are presented in the [Children's Guide to Kubernetes](https://www.cncf.io/phippy/) ([also in video](https://www.youtube.com/watch?v=4ht22ReBjno))_

In Kubernetes we have different kinds of resources that belong together. It could be multiple containers, or a service which has to find which pods to route to. We can use labels to group these resources together.

```yaml
metadata:
  name: my-application
  labels:
    app: company-website-app
    department: marketing
    release: stable
    version: 1.0.0
```

While this tells us more about the resource, it also allows us to select resources based on these labels.

```bash
kubectl get pods -l release=stable # get all pods with the label release=stable
```

Labels are also passed on into the resources Kubernetes creates for us.

They are often used in **Label Selectors** to find resources. For example, a Service can use a label selector to find the Pods it should route to.

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-application
spec:
    selector:
        app: company-website-app
        release: stable
    ports:
        - protocol: TCP
        port: 80
        targetPort: 80
```

This will make sure the service links to the app `company-website-app` with the label release version `stable`.

### Kubernetes Magic?

We have a spec written in YAML, we can use `kubectl` to create it in our cluster. But how does Kubernetes know what to do with it?
This is a process called **reconciling**.

![reconciling](./reconcile.jpg)

How to think of this? Well Kubernetes has a database full of YAML specs, and also access to the actual things it creates like containers.

It is in a constant infinite `while` loop that looks at the YAML specs and compares them to the actual object. If it doesn't exist it will create it. If it does exist it will make sure it is up to date with the `spec`. While doing this, it updates the `status` of the resource and sends out an event on what it did.

It will keep doing this process until all changes are satisfied. When reaching out to an external service like a cloud provider (eg. to request a disk) it will wait for the result in the background and will then continue the update process.

:::info
If you're a developer you immediately see the flaws. This is a simplified version. In reality the underlaying database [etcd](https://etcd.io/) offers something called "watchers" it will only run this code when something changes in the database. However it regulary checks if the actual state is the same as the desired state every few minutes/seconds.
:::

## Namespaces

You will find many namespaces in Kubernetes. They are used to group resources together and isolate them if needed. It allows for a better overview when looking into the cluster.

It also is a security boundry when setting up RBAC (Role Based Access Control), you can give a user or system access to a specific namespace.

We can list namespaces and make them when needed:

```bash
kubectl get namespaces
kubectl create namespace <namespace>
```

By default you will see the `default` and `kube-system` namespace, the last one is used by Kubernetes to host it's own internal system.

We can switch namespaces using the `-n` flag:

```bash
kubectl -n <namespace>
```

For example you might want to list the pods per namespace:

```bash
kubectl -n kube-system get pods
kubectl -n default get pods
```

:::tip
Some resources are never bound to a namespace like `nodes` or `persistentvolumes` and all resources starting with the prefix `cluster`.
Keep in mind these are exceptions and most resources are bound to a namespace.
:::

## Handy aliases

You can use aliases to make your life easier. For example you can use `k` instead of `kubectl`:

```bash
alias k=kubectl
alias kp='kubectl get pods'
alias kdp='kubectl describe pod'
```

The next one is `kcc`, this can be used to switch the default context (cluster) and namespace:

```bash
function kcc () {
    usage () {
        echo -en "Usage: $0 <context> <namespace>\n"
    }
    result () {
        echo -en "-> Context: \e[96m$context\e[0m\n-> Namespace: \e[92m$namespace\n\e[0m"
    }
    if  [ $# -eq 0 ] ; then
        ## If no options, print the current context/cluster and namespace
        context="$(kubectl config current-context 2>/dev/null)"
        namespace="$(kubectl config view -o "jsonpath={.contexts[?(@.name==\"$context\")].context.namespace}")"
        result
    elif [ $# -eq 2 ]; then
        ## If options, assume time to set
        context="$1"
        namespace="$2"
        kubectl config use-context "$context" > /dev/null
        kubectl config set-context "$context" --namespace="$namespace" > /dev/null
        result
    else
        usage
    fi

}
```

_Credits to [InAnimaTe](https://gist.github.com/InAnimaTe/eeeb4b01467d74c522b94f12ed009889)_

This will allow you to switch the cluster and namespace in one command:

```bash
kcc kind-kind kube-system # switch to the kind cluster and the kube-system namespace
kcc kind-kind default
```

**Add both these codes to your `~/.bashrc` file so they are ready for use.**

## kubeadm - the production ready official way

::: warning
kubeadm is mentioned to be complete but is out of scope for what we will do inside our classes. It can be used in your assignment as bonus points.
:::

[kubeadm](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/) is the community supported way to install Kubernetes. It is a great tool to install a production-ready Kubernetes cluster. It is a bit more complex to use than kind but it will get you started with a real cluster in no time.

To set it up you need:

- One "controller" node that has the Kubernetes API and the control plane
  - You can run multiple controllers (in uneven numbers) to have a highly available cluster
- One or more "worker" node that run the actual workloads

### Prepare the servers

Before we get started we need to prepare our Ubuntu servers to run Kubernetes and have containerd.

```bash
# load needed network kernel modules
cat <<EOF | sudo tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF

sudo modprobe overlay
sudo modprobe br_netfilter

# Setup required sysctl params, these persist across reboots.
cat <<EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF

# Apply sysctl params without reboot
sudo sysctl --system

# Install containerd (the container runtime)
sudo apt-get update && sudo apt-get install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo systemctl restart containerd
```

### The controller

The controller is the server that runs the Kubernetes API and the control plane. It is the server that you will connect to when you use `kubectl`.

You can install it with the following command:

```bash
sudo apt-get update && sudo apt-get install -y apt-transport-https ca-certificates curl
curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg
echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl # prevent auto-updates
```

You can then initialize the cluster with the following command:

```bash
kubeadm init --pod-network-cidr=172.16.0.0/12
```

This command will set up the cluster and print a command to run on the worker nodes to join the cluster.

### The workers

To set up a worker node you need a set up token from the controller. You can get it with the following command:

```bash
kubeadm token create --print-join-command
```

This will output a command that you can run on the worker nodes to join the cluster.

But first you need to install the same packages as on the controller:

```bash
sudo apt-get update && sudo apt-get install -y apt-transport-https ca-certificates curl
curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg
echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl # prevent auto-updates
```

When you have the command from the controller you can run it on the worker nodes to join the cluster.
You can check the status of the cluster with `kubectl get nodes`.

### Batteries not included

Kubeadm uses a very minimal set of components to run Kubernetes, this is so it is not opinionated on tools that are not part of the Kubernetes community. You will need to install additional components to get a full featured cluster.

#### Networking

This is an **essential\_** component of Kubernetes. Without a networking solution you will not be able to communicate between pods or let alone start them.

There are many networking solutions available, the most popular one is [Calico](https://www.projectcalico.org/). You can install it with the following command:

```bash
kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml
```

Calico is responsible for building an overlay network between the nodes in your cluster. It will also assign IP addresses to the pods and services in your cluster on this network. This network is only accessible from within the cluster.

#### Optional: LoadBalancer Services

If you want to expose your services ports to the outside world using a "floating IP" you will need a LoadBalancer. This is a component that will expose your services to the outside world. Your cloud provider will often have it's own LoadBalancer, but if you are running on bare metal you will need to install one yourself like [MetalLB](https://metallb.org/).

When installing MetalLB you will need to configure it with a range of IP addresses that it can use, once in use it will start answering ARP requests for these IP addresses and forward the traffic to the correct service.

#### Optional: Storage

Storage is yet another service that is typically provided by your cloud provider. Open source self-hosted solutions like [OpenEBS](https://openebs.io/) provide a way to run your own storage solution. It offers many options with different performance and redundancy ranges.

Unless you set up a production environment I recommend to try [Local PV Hostpath](https://openebs.io/docs/user-guides/localpv-hostpath) which will create a directory on the host and use that as a storage solution. **This is not a production ready solution** as it will assign it to only _one_ server, but it is easy to set up and will work for most use cases.

#### Ingress

Just like before with kind, we need our own ingress controller. The most popular one is [NGINX Ingress Controller](https://kubernetes.github.io/ingress-nginx/).

However when doing this you should consider several load balancing options... Everything you need is described in the [Bare-metal considerations guide](https://kubernetes.github.io/ingress-nginx/deploy/baremetal/).

#### The local multi VM cluster? Vagrant of course!

Want to get going with a multi server Kubernetes cluster on your laptop? Remember Vagrant from the first class of the year?
Yup there is a Vagrant setup for Kubernetes too! You can find it at [github.com/cloudnativehero/kubeadm-vagrant](https://github.com/cloudnativehero/kubeadm-vagrant)

## Want to become a real PRO?

(Do not do this... like really don't do this unless you do this for a living)

So you want to brag to your friends you know Kubernetes _really, really_ well? Go set up a cluster using [Kelsey Hightower's Kubernetes The Hard Way](https://github.com/kelseyhightower/kubernetes-the-hard-way).

[^bashbetter]: Eyskens, Mari√´n. (2022). Bash vs. PowerShell. B300 fights.
